# CLAUDE.md

This file provides guidance to Claude Code (claude.ai/code) when working with code in this repository.

## Project Overview

This repository implements compositional program synthesis via abstraction-refinement, featuring two main experimental domains:

1. **ARC-AGI Grid Puzzles** (`arc_dsl_experiment/`) - Eliminates symmetries through palette canonicalization and object ordering
2. **Inductive Programming** (`program_synthesis/`) - Cross-free factorization with interface refinement

The core insight is to lift problems to abstract domains, solve there, then embed back to concrete domains for exponential search space reduction.

## Common Commands

### Running Experiments

```bash
# Run ARC DSL experiments
cd arc_dsl_experiment && python3 dsl.py

# Run program synthesis scaling analysis  
cd program_synthesis && python3 scaling.py
```

### Working with LaTeX Papers

```bash
# Compile ARC abstractions paper
cd arc_dsl_experiment && pdflatex compositional_abstractions.tex

# Compile program synthesis paper
cd program_synthesis && pdflatex compositional_synthesis.tex
```

**Note:** LaTeX compilation requires `pdflatex` which may not be available in all environments. The compiled PDFs are included in the repository.

### Dependencies

The experiments require:
- Python 3.11+ (available as `python3`)
- NumPy (for `dsl.py`)
- Pandas and Matplotlib (for `scaling.py`)

Dependencies are not managed via package files - they need to be installed manually if missing.

## Architecture

### Core Abstraction Framework

The repository implements an abstraction-refinement tree where each node represents a task hypothesis with:
- **A**: abstract domain
- **e**: embedding A → G (concrete domain)  
- **m**: mapping of examples/test into A
- **S_A**: solver on A
- **score**: evidence for validity

### ARC DSL Implementation (`arc_dsl_experiment/dsl.py`)

- `alpha1_palette()` - A1 abstraction: palette canonicalization by frequency
- `alpha2_objorder()` - A2 abstraction: canonical object ordering  
- `segment_components()` - Connected component segmentation
- `ground_truth_transform()` - Task-specific transformation logic
- `measure_G()`, `measure_A1()`, `measure_A12()` - Performance measurement across abstraction levels

Key result: 2404→2 programs (-99.917%) with 138× speedup through composed abstractions.

### Program Synthesis Implementation (`program_synthesis/scaling.py`)

- `synthesize_global_bfs()` - Baseline global BFS search
- `compositional_plus()` - Two-phase approach: synthesize X-part, then enumerate cross-operation placements
- `build_target()` - Deterministic target program generation with configurable complexity
- Scaling analysis across Lx (program length), K (cross-operations), and branching factors

Key result: 4-59× fewer nodes, 8-184× speedup over global search.

## File Structure

```
arc_dsl_experiment/
  ├── dsl.py                          # Main DSL implementation
  ├── compositional_abstractions.tex  # Research paper source
  ├── compositional_abstractions.pdf  # Compiled paper
  ├── challenging_metrics.json        # Experimental results (JSON)
  └── challenging_metrics.txt         # Human-readable metrics

program_synthesis/
  ├── scaling.py                      # Scaling experiments
  ├── compositional_synthesis.tex     # Research paper source  
  ├── compositional_synthesis.pdf     # Compiled paper
  ├── nodes_vs_k_scaling.png         # Visualization outputs
  └── speedup_vs_k.png               # Generated by scaling.py
```

## Key Algorithms

- **Evidence scoring**: Training consistency, MDL compression, stability checks
- **Branching + voting**: Handle abstraction failures with K-best frontiers
- **Fallback mechanisms**: Revert to non-compositional baseline when abstractions fail

Both experiments demonstrate the core principle: when abstractions align with problem structure, dramatic efficiency gains are possible while maintaining correctness through systematic exploration and validation.